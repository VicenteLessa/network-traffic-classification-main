# network-traffic-classification

This repository contains the python scripts, csv and json files required to run the neural network for traffic classification.
The datasets used in this experiment are the same as the classic available at [1].

### Setup:

1- Clone this repository.

2- Upload the folder to your google drive main folder(If you want to use another directory scheme, just make sure to adequate the codes to point to the csv and json files).

3- Upload the python scripts to google colab. 

### Explanation about each python script:

The goal of the next two scripts is to generate a sorted list of attributes, ranked by the importance they should have in the NN. Each script have a different method to determine how important an attribute is. 

Feature Importance scripts:

1.1- NN feature importance using permutations.ipynb: This adapted algorithm described in [2] trains the NN with all attributes and test it a first time to have a reference accuracy score. Then, for each attribute, the test dataset multiples times applying the shuffling technique. The shuffling technique consists in choosing an attribute and shuffling it's values (by randomly switching places). The NN runs multiples times to have multiples shuffles for each attribute. A mean of the test accuracy score is taken for each shuffled attribute and compared to the reference score. The ideia is that more significant attributes should have a lower the score then less significant ones, because of the shuffling. Now we have a list of attributes corresponding to the sorted(crescent) accuracy scores.

1.2- NN feature importance using attributes weights.ipynb: In this NN, nodes are densely connected, this means that every normalized input node is connected to every node in the first hidden layer. The assumption used here is that nodes in the input layer with bigger (or lower!) connection weights should be more significant to the NN. This adapted algorithm described in [3] trains and tests the NN with all attributes. Then, for the first node, is made the sum of the connection weights between the normalized inputs layer and the first hidden layer. This is repeated for every node, resulting in a list of attributes associated with it's corresponding sum of connection weights. this list is crescent sorted by the connection weights.


Main Script:

2- NN with increasing number of attributes.ipynb: The NN is run with varying number of attributes being used (from number_of_attributes = 2  to number_of_attributes = max_number_of_attributes). The order in which the attributes are taken is the sorted list of attributes generated by either 1.1 or 1.2. The NN is trained and tested multiples times for each number of attributes being used. This is done to have more stable accuracy scores. This scores and other informations are dumped in json files for graph making.


Scripts for generating graphs:
3.1- IC NN Report Maker 01/10.ipynb: Test Accuracy by Number of Attributes using attributes connection weights.

3.2- IC NN Report Maker 02/05.ipynb: Test Accuracy by Number of Attributes using Permutation Shuffing.


### Running the experiment:
1- Once setup is done, Run one of the Feature Importance scripts.

2- Run the Main script

3- If you want to generated the graph of Test Accuracy by Number of Attributes for your run of the experiment, Adequate one the existing graph generating scripts to point to your experiment json report files.

[1]: https://www.cl.cam.ac.uk/research/srg/netos/projects/archive/nprobe/data/papers/sigmetrics/index.html

[2]: https://datascience.stackexchange.com/questions/44644/how-to-determine-feature-importance-in-a-neural-network

[3]: https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-912.pdf